import os
import re
import json
import csv
from pythainlp import word_tokenize
from pythainlp.util import dict_trie, num_to_thaiword
from pythainlp.corpus import thai_words
import config

# Global Variables
custom_tokenizer = None
my_custom_dict = {}
TEMP_MARKER = "###_NB_SPACE_###" # ‡∏Å‡∏≤‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏Ñ‡∏≥‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Ç‡∏≤‡∏î‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô

# Dictionary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
THAI_MONTHS = {
    '1': '‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°', '01': '‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°',
    '2': '‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå', '02': '‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå',
    '3': '‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°', '03': '‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°',
    '4': '‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô', '04': '‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô',
    '5': '‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°', '05': '‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°',
    '6': '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô', '06': '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô',
    '7': '‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°', '07': '‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°',
    '8': '‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°', '08': '‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°',
    '9': '‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô', '09': '‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô',
    '10': '‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°',
    '11': '‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô',
    '12': '‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°'
}

def load_custom_dict(file_path):
    custom_dict = {}
    if not os.path.exists(file_path):
        return {}
    if file_path.endswith('.json'):
        with open(file_path, 'r', encoding='utf-8') as f:
            custom_dict = json.load(f)
    elif file_path.endswith('.csv'):
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            for row in reader:
                if len(row) >= 2:
                    custom_dict[row[0].strip()] = row[1].strip()
    return custom_dict

def setup_tokenizer():
    global custom_tokenizer, my_custom_dict
    my_custom_dict = load_custom_dict(config.LEXICON_PATH)
    all_words = set(thai_words())
    all_words.update(my_custom_dict.keys())
    custom_tokenizer = dict_trie(all_words)
    print(f"‚úÖ Loaded Dictionary: {len(my_custom_dict)} words")

def replace_dates(text):
    """
    ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏û‡∏ó‡πÄ‡∏ó‡∏¥‡∏£‡πå‡∏ô DD/MM/YYYY ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏≠‡πà‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    Logic: ‡πÉ‡∏ä‡πâ TEMP_MARKER ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏Ñ‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡πâ‡∏≠‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡πÑ‡∏°‡πà‡πÇ‡∏î‡∏ô‡∏ï‡∏±‡∏î Space)
    ‡πÅ‡∏•‡∏∞‡πÉ‡∏™‡πà \n ‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏à‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
    """
    def date_replacer(match):
        d, m, y = match.groups()
        d_val = str(int(d)) 
        m_name = THAI_MONTHS.get(m, m) 
        
        # ‡πÉ‡∏ä‡πâ TEMP_MARKER ‡πÅ‡∏ó‡∏ô Space ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏°‡∏±‡∏î‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡πâ‡∏≠‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        # ‡πÄ‡∏ä‡πà‡∏ô: ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà###18###‡πÄ‡∏î‡∏∑‡∏≠‡∏ô###‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°###‡∏û‡∏∏‡∏ó‡∏ò‡∏®‡∏±‡∏Å‡∏£‡∏≤‡∏ä###2567
        return f"{TEMP_MARKER}{d_val}{TEMP_MARKER}{TEMP_MARKER}{m_name}{TEMP_MARKER}{TEMP_MARKER}{y}\n"

    pattern = r'\b(\d{1,2})[/-](\d{1,2})[/-](\d{4})\b'
    return re.sub(pattern, date_replacer, text)

def split_long_sentence(text, max_length=150):
    text = text.strip()
    if len(text) <= max_length:
        return [text]

    if custom_tokenizer is None: setup_tokenizer()
    words = word_tokenize(text, engine="newmm", custom_dict=custom_tokenizer)

    chunks = []
    current_chunk = ""

    for word in words:
        if len(current_chunk) + len(word) <= max_length:
            current_chunk += word
        else:
            if current_chunk: chunks.append(current_chunk)
            current_chunk = word

    if current_chunk:
        chunks.append(current_chunk)

    return chunks

def intelligent_split(text):
    if not text: return []
    text = text.strip()
    
    # 1. ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà (‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Å‡πâ‡∏≠‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô + \n)
    text = replace_dates(text)
    
    # 2. ‡∏•‡πá‡∏≠‡∏Å Space ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢-‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
    text = re.sub(r'(?<=[‡∏Å-‡πô])\s+(?=\d)', TEMP_MARKER, text)
    text = re.sub(r'(?<=\d)\s+(?=[‡∏Å-‡πô])', TEMP_MARKER, text)

    # 3. ‡∏ï‡∏±‡∏î‡∏î‡πâ‡∏ß‡∏¢ Newline (‡∏ã‡∏∂‡πà‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÇ‡∏î‡∏ô‡∏ï‡∏±‡∏î‡πÅ‡∏¢‡∏Å‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ)
    raw_chunks = re.split(r'[\n\r]+', text)
    
    final_segments = []
    for chunk in raw_chunks:
        # 4. ‡∏ï‡∏±‡∏î‡∏î‡πâ‡∏ß‡∏¢ Whitespace (‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢ Marker ‡πÑ‡∏ß‡πâ ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÇ‡∏î‡∏ô‡∏ï‡∏±‡∏î‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ)
        sub_chunks = re.split(r'\s+', chunk)
        for sub in sub_chunks:
            sub = sub.strip()
            if sub:
                # 5. ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Marker ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                restored = sub.replace(TEMP_MARKER, " ")
                
                # Check Length
                if len(restored) > 120:
                    micro_segments = split_long_sentence(restored, max_length=150)
                    final_segments.extend(micro_segments)
                else:
                    final_segments.append(restored)
                    
    return final_segments

def normalize_text(text):
    if custom_tokenizer is None:
        setup_tokenizer()
    
    # üî• Fix Preview: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å replace_dates ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÄ‡∏´‡πá‡∏ô‡∏Ñ‡∏≥‡∏≠‡πà‡∏≤‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
    text = replace_dates(text)
    # ‡∏•‡πâ‡∏≤‡∏á Marker ‡∏≠‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏Å‡∏ï‡∏¥ ‡∏Ñ‡∏ô‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏á‡∏á
    text = text.replace(TEMP_MARKER, " ")
    
    raw_tokens = word_tokenize(text, engine="newmm", custom_dict=custom_tokenizer)
    processed_tokens = []
    
    for token in raw_tokens:
        val = token
        
        if token in my_custom_dict:
            val = my_custom_dict[token]
        elif token.isdigit():
            try: val = num_to_thaiword(int(token))
            except: pass
        elif re.match(r'^([0-2]?[0-9])[:.]([0-5][0-9])$', token):
            try:
                parts = re.split(r'[:.]', token)
                hh, mm = int(parts[0]), int(parts[1])
                val = f"{num_to_thaiword(hh)}‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤"
                if mm > 0: val += f"{num_to_thaiword(mm)}‡∏ô‡∏≤‡∏ó‡∏µ"
            except: pass
        elif re.match(r'^\d+(\.\d+)?$', token):
             try: val = num_to_thaiword(float(token))
             except: pass
             
        val = val.strip()
        if val in ["‡∏ô.", "‡∏ô"]:
            continue
        if val: processed_tokens.append(val)
        
    final_text = " ".join(processed_tokens)
    return final_text, processed_tokens